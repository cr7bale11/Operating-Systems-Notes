**4 different Multi Processing architectures by Flynn:**
- Control Streams (Instructions) and Data Streams (Data)
- SISD: Single Instruction Single Data:
  - Basically, We have a single data and we do the single instruction on it (Single Processor)
    - for ex: A CPU wants to perform multiplication of a number by itself (finding the square of a number). 
      - Here, data is the number
      - Instruction is multiply it by itself
- SIMD: Single Instruction Multiple Data
  - Here we need to perform a single instrcution on a ***multiple data at any given time*** (how do we do that ? well, we have multiple cores and each core does the same instruction at any time)
    - For example, if we can do dot product simultaneously (multi each pair simultaneouly is an example of SIMD in the dot product case) 
      - Data here is multiple [1,2,3], [4,5,6] data here is {(1, 4), (2, 5), (3, 6)}
      - Instruction is single (we ignore the add here) -> Multiply two numbers
    - DOT Product is mainly used in Image Processing
    - GPUs use this to process instructions large amounts of data

- MISD: Multiple Instructions Single Data
  - Here, we perform multiple instructions on a single data
  - Doing two instructions at the same time on a single data (two processors trying to manipulate at the same time on a single data lol ðŸ¤£)
    - Is this feasible ??? No (can two cores do squaring and cubing at a same time on a single data (number ?)... Consider eating and sleeping soundly at the same time. Will it work ?in the realworld, no)
- MIMD: Multiple Instructions Multiple Data
  - Here, we try to perform multiple instructions on multiple data points (HIghly feasible and its the most commonly used)
    - Example: Computer processing the songs to listen to and while trying to run a c++ program 
    - Two more subdivisions:
      - SPMD (Single Program Multiple Data)
        - Executes the same copy of the program on multiple/different data (different from SIMD as in SIMD, all the instructions must run at the same time)
        - MPMD (Multiple Programs Multiple Data) (Donno)

**Memory**:
  - Memory reads/writes are much slower than CPU processing (3200Mhz vs 3Ghz)
  - So, A processor trying to update a cell in the mmemory will block that cell completely from other processors 
  - Shared vs Disributive Memory
    - Shared:
      - All the processors access the same memory thats' in the global address space i.e.. altho each processor accesses the memory independently, any updates on the memory will be known to all the processors as the memory is global
        - Example: Copying data in the memory (basically copying into two) will be known to all the processors
      - The memory maynot be existing on the same physical device (like a cluster of systems)
      - The key thing is that all the processors see the changes that happens in the memory
      - UMA/NUMA (Uniform Memory Access/Non Uniform Memory Access) designs:
        - UMA: In this design, All of the processors have equal access to the memory (equal latency to access the memory)
          - Most common architecture in UMA (Symmetric Multirprocessing System/SMP) -> 
            - In this architecture, all the processors connect to the main memory via the system bus. 
              - One issue that we can have is as each processor (currently) has local cache (L1, L2 caches are at each core). 
              - So, If there's an update in the main memory for a key that's present in the cache of a processor, the cache data must also be updated... (even tho the processor sees the update, maybe it's unable to do anything), This issue is called **Cache Coherance**
        - Non Uniform Memory Access Design: 
          - It's a hub of UMAs (think of a system that has multiple UMAs)
          - So each node of NUMA will have a memory, a bus inside and also some CPUs. There can be multiple nodes. And each node is connected to a global bus
            - So, For some memory access, If it's in their local node's memory, then it'll be quick or else, it'll be slow.
            - As in shared memory, every processor in every node can still see the memory changes (lol, I donno how)
      - Doesn't scale well even tho it's easy for the programmers to write llel code. For scaling, If new processors are added, then because of Cache Coherence (one such example for downsides), to update the data in the local caches, there must be a lot of communication to happen
      - Shared Memory puts the responsibility on the programmer to synchronize the memory access for correct behavior
    - Distributed memory:
      - No concept of global memory. Each node/processor has their own local memory for access. And all of these processors are connected via a network like an Ethernet connection.
      - If there's a change in one of the node's local memory, all the other nodes are oblivious to this change
      - Communication is still tough but it's scalable as adding more nodes doesn't affect the system at all
    - Super Computers use some form of Distributed Memory Architecture/Design or a combo of both 
    - In the real world, the number of processors inside a chip (CPU) is ever growing with in a span of 3 years (from i7-5th gen to i5-8th gen), the number of processors has been more than doubled. And As ours is not a distrubuted systems based models (laptops, desktops, mobiles), the main limitation comes from is accessing the memory itself as it's global. 
      - So, adding processors doesn't really work ideally as it will lead to more communication on the memory via the bus and it'll decrease the performance instead. So, what did Intel and AMD do to overcome the memory overhead and add more cores ???